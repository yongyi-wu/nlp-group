Emotion detection is one of the fundamental tasks for more advanced applications of Natural Language Processing and Human-Computer Interaction, ranging from dialog systems to spam detection. This task aims to assign emotion labels that could be easily understood by humans to given input texts. 

GoEmotions is a recent work that contains by far the largest manually annotated data of 58k English Reddit comments, labeled for 27 emotion categories or ``neutral'' \citet{demszky2020goemotions}. The baseline model proposed by the authors follows the conventional classification regime, where a pretrained language model (in this case, cased \textsc{Bert Base}) extracts the contextual representation of \texttt{[CLS]} token, which is then fed into a multilayer perceptron (MLP) to compute, for each emotion, the probability of existence in the given text. However, the fine-grained taxonomy in the GoEmotions dataset presents challenge for the baseline classifier to distinguish nuanced emotions, such as anger and annoyance, so the \texttt{[CLS]} token alone may not be able to carry enough information to yield desired performance across different labels. 

In this work, we propose a label-aware attention mechanism where each emotion (classification label) maintains its own latent representation, and uses it to query and aggregate the contextual representation of all tokens from the input text. By computing attention distribution for each emotion independently, we hope the semantic information carried by the label would guide the classifier to focus on different aspects of the same text. Our method is reminiscent of AttentionXML (\citet{you2019attentionxml}) and LEAM (\citet{wang2018joint}) but is much easier to implement and train by avoiding the probabilistic label tree or the temporal convolution. Our model is trained under the class-balanced loss proposed by  \citet{cui2019classbalanced}, which helped alleviate the uneven distribution of emotion labels in the GoEmotions dataset. 

% We have also tried another label-aware mechanism, which is inspired by the LEAM architecture (Wang et al. 2018). The detail of this model is presented in section \ref{rel}. The most significant distinction from our proposed model is this model's different computational approach towards making the classifier attending to different emotional aspects of the input text. By the time of submission, our proposed method wins out with the best performance in terms of overall macro-F1 score (the LEAM results are also presented in Section \ref{res}) and individual classes F1 scores. 

The paper is organized as follows: Section \ref{method} introduces the design of our proposed classifier. We then present experiment details Section \ref{exp} and show our method performs significantly better than the baseline. After that, we review related work in emotion detection in Section \ref{rel} and conclude our work with a discussion in Section \ref{conc}. 
